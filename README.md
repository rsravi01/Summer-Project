# ğŸ Software Bug Prediction using Semi-Supervised Learning

This project implements a semi-supervised learning approach to predict software bugs using the KC1 defect dataset from the PROMISE repository. It combines ensemble models with pseudo-labeling to leverage both labeled and unlabeled data, improving prediction performance under limited supervision.

---

## ğŸš€ Overview

- Developed a **semi-supervised machine learning pipeline** to detect defects in software modules.
- Applied **SMOTE** to handle class imbalance and **StandardScaler** for feature normalization.
- Used **Random Forest feature importance** for feature selection, achieving 88% accuracy on selected features.
- Combined **Random Forest, XGBoost, SVM, and Logistic Regression** in a soft-voting ensemble.
- Employed **iterative pseudo-labeling** with confidence thresholds (0.80â€“0.90) and **early stopping** based on validation F1-score stagnation.

---

## ğŸ§± System Architecture

The following pipeline summarizes the semi-supervised defect prediction architecture:

<ğŸ“„ Data Input
   â”‚
   â–¼
ğŸ§ª Preprocessing
(StandardScaler + SMOTE)
   â”‚
   â–¼
ğŸ“Š Feature Selection
(Random Forest Feature Importance)
   â”‚
   â–¼
ğŸ” Iterative Training Loop
   â”‚
   â”œâ”€â”€â–º Train Ensemble Model
   â”‚       â”œâ”€â”€ Random Forest
   â”‚       â”œâ”€â”€ XGBoost
   â”‚       â”œâ”€â”€ SVM
   â”‚       â””â”€â”€ Logistic Regression
   â”‚
   â”œâ”€â”€â–º Predict Unlabeled Samples
   â”œâ”€â”€â–º Select High-Confidence Pseudo Labels (Threshold: 0.80â€“0.90)
   â””â”€â”€â–º Augment Training Set
   â”‚
   â–¼
â¹ Early Stopping
(Monitor Validation F1 Stagnation)
   â”‚
   â–¼
âœ… Final Evaluation
(Accuracy, Precision, Recall, F1-Score)
>
![SummerProject](https://github.com/user-attachments/assets/ed182b8b-6b28-4a1f-ba91-85ad84a05866)

---

## ğŸ“Š Results

- Explored labeled data ratios from **10% to 90%**.
- Achieved peak test performance:
  - **Accuracy:** 88.44%
  - **F1-Score:** 0.8451
  - **Precision:** 0.8491
  - **Recall:** 0.8411
- Demonstrated strong generalization using semi-supervised learning under label-scarce conditions.

---

## ğŸ§° Tools & Technologies

- **Languages**: Python
- **Libraries**:  
  - `scikit-learn` (RandomForest, SVC, LogisticRegression, StandardScaler, metrics)  
  - `XGBoost`  
  - `imbalanced-learn` (SMOTE)  
  - `Pandas`, `NumPy`  
  - `VotingClassifier`

---

## ğŸ—‚ Dataset

- **KC1** defect dataset from the [PROMISE repository](http://promise.site.uottawa.ca/SERepository/datasets-page.html)
- Preprocessing steps:
  - Handling missing values
  - Feature normalization
  - Binary classification based on `defects` field

---

## ğŸ§  Methodology

1. **Preprocessing**  
   Normalize and balance data using `StandardScaler` and `SMOTE`.

2. **Feature Selection**  
   Use Random Forest importance scores to select top predictors.

3. **Model Training**  
   Train ensemble on labeled samples using soft-voting.

4. **Pseudo-labeling**  
   Add confidently predicted unlabeled samples (â‰¥ threshold) to the training set iteratively.

5. **Early Stopping**  
   Monitor validation F1; stop if no improvement in 3 epochs.

6. **Evaluation**  
   Assess final model using test set performance metrics.

---

## ğŸ“ Files

- `software_bug_prediction.py`: Full implementation.
- `kc1.csv`: Input dataset.
- `README.md`: Documentation.

---

## ğŸ™ Acknowledgements

This project was conducted under the guidance of **Ms. Vandana Bhattacharya** as part of the **Summer Training Program** at *Birla Institute of Technology, Mesra*.  
We are thankful to the **PROMISE repository** for providing access to the KC1 software defect dataset used in this work.


