{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfkYcri6mJ8m",
        "outputId": "ef575a5f-fb7f-429e-9409-c7dededec78d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Table I: Summary of Best Threshold per Ratio\n",
            "\n",
            "   Labeled-Unlabeled Ratio   Sup_Acc  Sup_Prec  Sup_Recall    Sup_F1  \\\n",
            "0                      0.1  0.843602  0.489583    0.361538  0.415929   \n",
            "1                      0.2  0.861374  0.555556    0.500000  0.526316   \n",
            "2                      0.3  0.870853  0.596330    0.500000  0.543933   \n",
            "3                      0.4  0.873223  0.596639    0.546154  0.570281   \n",
            "4                      0.5  0.874408  0.596774    0.569231  0.582677   \n",
            "5                      0.6  0.870853  0.576642    0.607692  0.591760   \n",
            "6                      0.7  0.877962  0.609756    0.576923  0.592885   \n",
            "7                      0.8  0.886256  0.639344    0.600000  0.619048   \n",
            "8                      0.9  0.880332  0.612403    0.607692  0.610039   \n",
            "\n",
            "   Sup_ROC_AUC  Best Threshold  Best_Semi_Acc  Best_Semi_Prec  \\\n",
            "0     0.806249            0.85       0.859005        0.569620   \n",
            "1     0.836598            0.90       0.859005        0.549550   \n",
            "2     0.853674            0.85       0.883886        0.666667   \n",
            "3     0.849391            0.90       0.874408        0.617647   \n",
            "4     0.851002            0.90       0.869668        0.581967   \n",
            "5     0.857741            0.70       0.882701        0.622047   \n",
            "6     0.885186            0.80       0.886256        0.646552   \n",
            "7     0.895335            0.75       0.895735        0.669355   \n",
            "8     0.898190            0.90       0.896919        0.669291   \n",
            "\n",
            "   Best_Semi_Recall  Best_Semi_F1  Best_Semi_ROC_AUC  \n",
            "0          0.346154      0.430622           0.801395  \n",
            "1          0.469231      0.506224           0.845755  \n",
            "2          0.492308      0.566372           0.853954  \n",
            "3          0.484615      0.543103           0.847506  \n",
            "4          0.546154      0.563492           0.843708  \n",
            "5          0.607692      0.614786           0.850970  \n",
            "6          0.576923      0.609756           0.888084  \n",
            "7          0.638462      0.653543           0.890853  \n",
            "8          0.653846      0.661479           0.901821  \n",
            "\n",
            "Table II: All Threshold Results\n",
            "\n",
            "    Labeled-Unlabeled Ratio  Threshold  Supervised Acc  Supervised Prec  \\\n",
            "0                       0.1       0.90        0.843602         0.489583   \n",
            "1                       0.1       0.85        0.843602         0.489583   \n",
            "2                       0.1       0.80        0.843602         0.489583   \n",
            "3                       0.1       0.75        0.843602         0.489583   \n",
            "4                       0.1       0.70        0.843602         0.489583   \n",
            "5                       0.1       0.65        0.843602         0.489583   \n",
            "6                       0.2       0.90        0.861374         0.555556   \n",
            "7                       0.2       0.85        0.861374         0.555556   \n",
            "8                       0.2       0.80        0.861374         0.555556   \n",
            "9                       0.2       0.75        0.861374         0.555556   \n",
            "10                      0.2       0.70        0.861374         0.555556   \n",
            "11                      0.2       0.65        0.861374         0.555556   \n",
            "12                      0.3       0.90        0.870853         0.596330   \n",
            "13                      0.3       0.85        0.870853         0.596330   \n",
            "14                      0.3       0.80        0.870853         0.596330   \n",
            "15                      0.3       0.75        0.870853         0.596330   \n",
            "16                      0.3       0.70        0.870853         0.596330   \n",
            "17                      0.3       0.65        0.870853         0.596330   \n",
            "18                      0.4       0.90        0.873223         0.596639   \n",
            "19                      0.4       0.85        0.873223         0.596639   \n",
            "20                      0.4       0.80        0.873223         0.596639   \n",
            "21                      0.4       0.75        0.873223         0.596639   \n",
            "22                      0.4       0.70        0.873223         0.596639   \n",
            "23                      0.4       0.65        0.873223         0.596639   \n",
            "24                      0.5       0.90        0.874408         0.596774   \n",
            "25                      0.5       0.85        0.874408         0.596774   \n",
            "26                      0.5       0.80        0.874408         0.596774   \n",
            "27                      0.5       0.75        0.874408         0.596774   \n",
            "28                      0.5       0.70        0.874408         0.596774   \n",
            "29                      0.5       0.65        0.874408         0.596774   \n",
            "30                      0.6       0.90        0.870853         0.576642   \n",
            "31                      0.6       0.85        0.870853         0.576642   \n",
            "32                      0.6       0.80        0.870853         0.576642   \n",
            "33                      0.6       0.75        0.870853         0.576642   \n",
            "34                      0.6       0.70        0.870853         0.576642   \n",
            "35                      0.6       0.65        0.870853         0.576642   \n",
            "36                      0.7       0.90        0.877962         0.609756   \n",
            "37                      0.7       0.85        0.877962         0.609756   \n",
            "38                      0.7       0.80        0.877962         0.609756   \n",
            "39                      0.7       0.75        0.877962         0.609756   \n",
            "40                      0.7       0.70        0.877962         0.609756   \n",
            "41                      0.7       0.65        0.877962         0.609756   \n",
            "42                      0.8       0.90        0.886256         0.639344   \n",
            "43                      0.8       0.85        0.886256         0.639344   \n",
            "44                      0.8       0.80        0.886256         0.639344   \n",
            "45                      0.8       0.75        0.886256         0.639344   \n",
            "46                      0.8       0.70        0.886256         0.639344   \n",
            "47                      0.8       0.65        0.886256         0.639344   \n",
            "48                      0.9       0.90        0.880332         0.612403   \n",
            "49                      0.9       0.85        0.880332         0.612403   \n",
            "50                      0.9       0.80        0.880332         0.612403   \n",
            "51                      0.9       0.75        0.880332         0.612403   \n",
            "52                      0.9       0.70        0.880332         0.612403   \n",
            "53                      0.9       0.65        0.880332         0.612403   \n",
            "\n",
            "    Supervised Recall  Supervised F1  Supervised ROC_AUC  Semi Acc  Semi Prec  \\\n",
            "0            0.361538       0.415929            0.806249  0.851896   0.531646   \n",
            "1            0.361538       0.415929            0.806249  0.859005   0.569620   \n",
            "2            0.361538       0.415929            0.806249  0.854265   0.552239   \n",
            "3            0.361538       0.415929            0.806249  0.857820   0.575758   \n",
            "4            0.361538       0.415929            0.806249  0.855450   0.571429   \n",
            "5            0.361538       0.415929            0.806249  0.855450   0.571429   \n",
            "6            0.500000       0.526316            0.836598  0.859005   0.549550   \n",
            "7            0.500000       0.526316            0.836598  0.862559   0.568627   \n",
            "8            0.500000       0.526316            0.836598  0.857820   0.554348   \n",
            "9            0.500000       0.526316            0.836598  0.856635   0.547368   \n",
            "10           0.500000       0.526316            0.836598  0.856635   0.542857   \n",
            "11           0.500000       0.526316            0.836598  0.855450   0.537736   \n",
            "12           0.500000       0.543933            0.853674  0.872038   0.610000   \n",
            "13           0.500000       0.543933            0.853674  0.883886   0.666667   \n",
            "14           0.500000       0.543933            0.853674  0.877962   0.639175   \n",
            "15           0.500000       0.543933            0.853674  0.875592   0.637363   \n",
            "16           0.500000       0.543933            0.853674  0.868483   0.604396   \n",
            "17           0.500000       0.543933            0.853674  0.869668   0.606383   \n",
            "18           0.546154       0.570281            0.849391  0.874408   0.617647   \n",
            "19           0.546154       0.570281            0.849391  0.872038   0.603774   \n",
            "20           0.546154       0.570281            0.849391  0.873223   0.613861   \n",
            "21           0.546154       0.570281            0.849391  0.877962   0.642105   \n",
            "22           0.546154       0.570281            0.849391  0.874408   0.633333   \n",
            "23           0.546154       0.570281            0.849391  0.873223   0.623656   \n",
            "24           0.569231       0.582677            0.851002  0.869668   0.581967   \n",
            "25           0.569231       0.582677            0.851002  0.868483   0.585586   \n",
            "26           0.569231       0.582677            0.851002  0.876777   0.625000   \n",
            "27           0.569231       0.582677            0.851002  0.877962   0.642105   \n",
            "28           0.569231       0.582677            0.851002  0.874408   0.625000   \n",
            "29           0.569231       0.582677            0.851002  0.877962   0.645161   \n",
            "30           0.607692       0.591760            0.857741  0.869668   0.574627   \n",
            "31           0.607692       0.591760            0.857741  0.870853   0.578947   \n",
            "32           0.607692       0.591760            0.857741  0.874408   0.590909   \n",
            "33           0.607692       0.591760            0.857741  0.876777   0.601562   \n",
            "34           0.607692       0.591760            0.857741  0.882701   0.622047   \n",
            "35           0.607692       0.591760            0.857741  0.879147   0.604478   \n",
            "36           0.576923       0.592885            0.885186  0.876777   0.603175   \n",
            "37           0.576923       0.592885            0.885186  0.876777   0.606557   \n",
            "38           0.576923       0.592885            0.885186  0.886256   0.646552   \n",
            "39           0.576923       0.592885            0.885186  0.882701   0.630252   \n",
            "40           0.576923       0.592885            0.885186  0.882701   0.628099   \n",
            "41           0.576923       0.592885            0.885186  0.882701   0.626016   \n",
            "42           0.600000       0.619048            0.895335  0.894550   0.669421   \n",
            "43           0.600000       0.619048            0.895335  0.893365   0.661290   \n",
            "44           0.600000       0.619048            0.895335  0.894550   0.666667   \n",
            "45           0.600000       0.619048            0.895335  0.895735   0.669355   \n",
            "46           0.600000       0.619048            0.895335  0.890995   0.655738   \n",
            "47           0.600000       0.619048            0.895335  0.887441   0.649573   \n",
            "48           0.607692       0.610039            0.898190  0.896919   0.669291   \n",
            "49           0.607692       0.610039            0.898190  0.885071   0.629921   \n",
            "50           0.607692       0.610039            0.898190  0.889810   0.648000   \n",
            "51           0.607692       0.610039            0.898190  0.887441   0.635659   \n",
            "52           0.607692       0.610039            0.898190  0.883886   0.623077   \n",
            "53           0.607692       0.610039            0.898190  0.887441   0.637795   \n",
            "\n",
            "    Semi Recall   Semi F1  Semi ROC_AUC  \n",
            "0      0.323077  0.401914      0.804746  \n",
            "1      0.346154  0.430622      0.801395  \n",
            "2      0.284615  0.375635      0.779498  \n",
            "3      0.292308  0.387755      0.769716  \n",
            "4      0.246154  0.344086      0.772614  \n",
            "5      0.246154  0.344086      0.763785  \n",
            "6      0.469231  0.506224      0.845755  \n",
            "7      0.446154  0.500000      0.842658  \n",
            "8      0.392308  0.459459      0.833107  \n",
            "9      0.400000  0.462222      0.816133  \n",
            "10     0.438462  0.485106      0.820906  \n",
            "11     0.438462  0.483051      0.816591  \n",
            "12     0.469231  0.530435      0.852785  \n",
            "13     0.492308  0.566372      0.853954  \n",
            "14     0.476923  0.546256      0.853211  \n",
            "15     0.446154  0.524887      0.848944  \n",
            "16     0.423077  0.497738      0.840250  \n",
            "17     0.438462  0.508929      0.837233  \n",
            "18     0.484615  0.543103      0.847506  \n",
            "19     0.492308  0.542373      0.847818  \n",
            "20     0.476923  0.536797      0.849785  \n",
            "21     0.469231  0.542222      0.850657  \n",
            "22     0.438462  0.518182      0.841193  \n",
            "23     0.446154  0.520179      0.838882  \n",
            "24     0.546154  0.563492      0.843708  \n",
            "25     0.500000  0.539419      0.844268  \n",
            "26     0.500000  0.555556      0.843800  \n",
            "27     0.469231  0.542222      0.839086  \n",
            "28     0.461538  0.530973      0.838688  \n",
            "29     0.461538  0.538117      0.837557  \n",
            "30     0.592308  0.583333      0.857229  \n",
            "31     0.592308  0.585551      0.855365  \n",
            "32     0.600000  0.595420      0.857574  \n",
            "33     0.592308  0.596899      0.855155  \n",
            "34     0.607692  0.614786      0.850970  \n",
            "35     0.623077  0.613636      0.847840  \n",
            "36     0.584615  0.593750      0.882601  \n",
            "37     0.569231  0.587302      0.880037  \n",
            "38     0.576923  0.609756      0.888084  \n",
            "39     0.576923  0.602410      0.882816  \n",
            "40     0.584615  0.605578      0.882116  \n",
            "41     0.592308  0.608696      0.879466  \n",
            "42     0.623077  0.645418      0.897447  \n",
            "43     0.630769  0.645669      0.891198  \n",
            "44     0.630769  0.648221      0.893428  \n",
            "45     0.638462  0.653543      0.890853  \n",
            "46     0.615385  0.634921      0.890961  \n",
            "47     0.584615  0.615385      0.891575  \n",
            "48     0.653846  0.661479      0.901821  \n",
            "49     0.615385  0.622568      0.899030  \n",
            "50     0.623077  0.635294      0.901401  \n",
            "51     0.630769  0.633205      0.902564  \n",
            "52     0.623077  0.623077      0.899117  \n",
            "53     0.623077  0.630350      0.899957  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('kc1.csv')\n",
        "df.fillna(df.median(), inplace=True)\n",
        "df['defects'] = df['defects'].astype(int)\n",
        "\n",
        "X = df.drop(columns=['defects'])\n",
        "y = df['defects']\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Data Augmentation: Add Gaussian Noise\n",
        "def add_noise(data, noise_level=0.01):\n",
        "    noise = np.random.normal(0, noise_level, data.shape)\n",
        "    return data + noise\n",
        "\n",
        "X_augmented = add_noise(X_scaled)\n",
        "X_scaled = pd.concat([X_scaled, X_augmented], ignore_index=True)\n",
        "y = pd.concat([y, y], ignore_index=True)\n",
        "\n",
        "# SMOTE helper\n",
        "def apply_smote(X_train, y_train):\n",
        "    smote = SMOTE(sampling_strategy=0.6, random_state=42)\n",
        "    return smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Evaluation metric helper\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    preds = model.predict(X_test)\n",
        "    probs = model.predict_proba(X_test)[:, 1]\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, preds),\n",
        "        \"Precision\": precision_score(y_test, preds),\n",
        "        \"Recall\": recall_score(y_test, preds),\n",
        "        \"F1\": f1_score(y_test, preds),\n",
        "        \"ROC-AUC\": roc_auc_score(y_test, probs)\n",
        "    }\n",
        "\n",
        "# Base Classifier (easy to swap)\n",
        "def get_classifier(class_weight_dict):\n",
        "    return RandomForestClassifier(n_estimators=200, max_depth=12, class_weight=class_weight_dict, random_state=42)\n",
        "    # return LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
        "    # return XGBClassifier(scale_pos_weight=class_weight_dict[0]/class_weight_dict[1], use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Thresholds to try\n",
        "thresholds = [0.9, 0.85, 0.8, 0.75, 0.7, 0.65]\n",
        "\n",
        "# Split ratios\n",
        "split_ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "table1_results = []\n",
        "table2_results = []\n",
        "\n",
        "for ratio in split_ratios:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
        "    X_train_labeled, X_unlabeled, y_train_labeled, y_unlabeled = train_test_split(\n",
        "        X_train, y_train, test_size=(1 - ratio), stratify=y_train, random_state=42)\n",
        "\n",
        "    X_train_labeled, y_train_labeled = apply_smote(X_train_labeled, y_train_labeled)\n",
        "    class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train_labeled), y=y_train_labeled)\n",
        "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "    clf = get_classifier(class_weight_dict)\n",
        "    clf.fit(X_train_labeled, y_train_labeled)\n",
        "\n",
        "    supervised_metrics = evaluate_model(clf, X_test, y_test)\n",
        "    best_f1 = 0\n",
        "    best_metrics = {}\n",
        "    best_threshold = None\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        X_train_final = X_train_labeled.copy()\n",
        "        y_train_final = y_train_labeled.copy()\n",
        "        X_unlabeled_copy = X_unlabeled.copy()\n",
        "        patience, max_patience = 0, 3\n",
        "        prev_f1 = 0\n",
        "\n",
        "        for _ in range(10):  # Max 10 rounds\n",
        "            probs = clf.predict_proba(X_unlabeled_copy)\n",
        "            max_probs = np.max(probs, axis=1)\n",
        "            pseudo_labels = np.argmax(probs, axis=1)\n",
        "            high_confidence_idx = np.where(max_probs >= threshold)[0]\n",
        "\n",
        "            if len(high_confidence_idx) == 0:\n",
        "                break\n",
        "\n",
        "            # Add high confidence pseudo-labels\n",
        "            X_train_final = pd.concat(\n",
        "                [pd.DataFrame(X_train_final, columns=X.columns), X_unlabeled_copy.iloc[high_confidence_idx]],\n",
        "                ignore_index=True\n",
        "            )\n",
        "            y_train_final = np.concatenate((y_train_final, pseudo_labels[high_confidence_idx]))\n",
        "\n",
        "            # Drop used samples\n",
        "            X_unlabeled_copy = X_unlabeled_copy.drop(X_unlabeled_copy.index[high_confidence_idx])\n",
        "\n",
        "            # Shuffle and retrain\n",
        "            X_train_final, y_train_final = shuffle(X_train_final, y_train_final, random_state=42)\n",
        "            clf.fit(X_train_final, y_train_final)\n",
        "            current_f1 = f1_score(y_test, clf.predict(X_test))\n",
        "\n",
        "            # Early stopping if no improvement\n",
        "            if current_f1 <= prev_f1:\n",
        "                patience += 1\n",
        "            else:\n",
        "                patience = 0\n",
        "                prev_f1 = current_f1\n",
        "\n",
        "            if patience >= max_patience:\n",
        "                break\n",
        "\n",
        "        semi_metrics = evaluate_model(clf, X_test, y_test)\n",
        "        table2_results.append([ratio, threshold, *supervised_metrics.values(), *semi_metrics.values()])\n",
        "\n",
        "        if semi_metrics[\"F1\"] > best_f1:\n",
        "            best_f1 = semi_metrics[\"F1\"]\n",
        "            best_metrics = semi_metrics\n",
        "            best_threshold = threshold\n",
        "\n",
        "    table1_results.append([ratio, *supervised_metrics.values(), best_threshold, *best_metrics.values()])\n",
        "\n",
        "# Save to DataFrames\n",
        "table1_df = pd.DataFrame(table1_results, columns=[\n",
        "    \"Labeled-Unlabeled Ratio\", \"Sup_Acc\", \"Sup_Prec\", \"Sup_Recall\", \"Sup_F1\", \"Sup_ROC_AUC\",\n",
        "    \"Best Threshold\", \"Best_Semi_Acc\", \"Best_Semi_Prec\", \"Best_Semi_Recall\", \"Best_Semi_F1\", \"Best_Semi_ROC_AUC\"\n",
        "])\n",
        "\n",
        "table2_df = pd.DataFrame(table2_results, columns=[\n",
        "    \"Labeled-Unlabeled Ratio\", \"Threshold\",\n",
        "    \"Supervised Acc\", \"Supervised Prec\", \"Supervised Recall\", \"Supervised F1\", \"Supervised ROC_AUC\",\n",
        "    \"Semi Acc\", \"Semi Prec\", \"Semi Recall\", \"Semi F1\", \"Semi ROC_AUC\"\n",
        "])\n",
        "\n",
        "# Display\n",
        "print(\"\\nTable I: Summary of Best Threshold per Ratio\\n\")\n",
        "print(table1_df)\n",
        "\n",
        "print(\"\\nTable II: All Threshold Results\\n\")\n",
        "print(table2_df)\n",
        "\n",
        "# Optional: Save to CSV\n",
        "table1_df.to_csv(\"table1_summary.csv\", index=False)\n",
        "table2_df.to_csv(\"table2_thresholds.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from imblearn.combine import SMOTETomek\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('kc1.csv')\n",
        "data.dropna(inplace=True)\n",
        "X = data.drop(['defects'], axis=1).values\n",
        "y = data['defects'].values\n",
        "\n",
        "# Resample using SMOTE-Tomek\n",
        "smt = SMOTETomek(random_state=42)\n",
        "X_resampled, y_resampled = smt.fit_resample(X, y)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_resampled = scaler.fit_transform(X_resampled)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# GA + ANN functions\n",
        "def create_model(input_dim, hidden_units, learning_rate, dropout_rate):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(hidden_units, activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(hidden_units // 2, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def GA_tune(X, y, population_size=6, generations=5):\n",
        "    input_dim = X.shape[1]\n",
        "\n",
        "    # Extended search space\n",
        "    hidden_units_list = [32, 64, 128, 256]\n",
        "    learning_rates = [0.0005, 0.001, 0.0025, 0.005, 0.01]\n",
        "    batch_sizes = [16, 32, 64, 128]\n",
        "    epochs_list = [20, 30, 40]\n",
        "    dropout_rates = [0.1, 0.2, 0.3, 0.4]\n",
        "\n",
        "    def random_individual():\n",
        "        return {\n",
        "            'hidden_units': random.choice(hidden_units_list),\n",
        "            'learning_rate': random.choice(learning_rates),\n",
        "            'batch_size': random.choice(batch_sizes),\n",
        "            'epochs': random.choice(epochs_list),\n",
        "            'dropout_rate': random.choice(dropout_rates)\n",
        "        }\n",
        "\n",
        "    def fitness(indiv):\n",
        "        model = create_model(input_dim, indiv['hidden_units'], indiv['learning_rate'], indiv['dropout_rate'])\n",
        "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        model.fit(X_train, y_train,\n",
        "                  epochs=indiv['epochs'],\n",
        "                  batch_size=indiv['batch_size'],\n",
        "                  verbose=0,\n",
        "                  validation_split=0.2,\n",
        "                  callbacks=[early_stop])\n",
        "        _, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "        return acc\n",
        "\n",
        "    # Initial population\n",
        "    population = [random_individual() for _ in range(population_size)]\n",
        "\n",
        "    for gen in range(generations):\n",
        "        print(f\"\\nGeneration {gen+1}\")\n",
        "        fitness_scores = [fitness(indiv) for indiv in population]\n",
        "        sorted_pop = [x for _, x in sorted(zip(fitness_scores, population), key=lambda pair: pair[0], reverse=True)]\n",
        "\n",
        "        new_population = sorted_pop[:2]  # Elitism\n",
        "        while len(new_population) < population_size:\n",
        "            p1, p2 = random.sample(sorted_pop[:4], 2)\n",
        "            child = {\n",
        "                'hidden_units': random.choice([p1['hidden_units'], p2['hidden_units']]),\n",
        "                'learning_rate': random.choice([p1['learning_rate'], p2['learning_rate']]),\n",
        "                'batch_size': random.choice([p1['batch_size'], p2['batch_size']]),\n",
        "                'epochs': random.choice([p1['epochs'], p2['epochs']]),\n",
        "                'dropout_rate': random.choice([p1['dropout_rate'], p2['dropout_rate']])\n",
        "            }\n",
        "            # Mutation\n",
        "            if random.random() < 0.2:\n",
        "                child['hidden_units'] = random.choice(hidden_units_list)\n",
        "            new_population.append(child)\n",
        "\n",
        "        population = new_population\n",
        "\n",
        "    best_individual = population[0]\n",
        "    print(\"\\nBest Parameters from GA:\", best_individual)\n",
        "    return best_individual\n",
        "\n",
        "# Run GA to find best parameters\n",
        "best_params = GA_tune(X_resampled, y_resampled)\n",
        "\n",
        "# Train final model using best params\n",
        "model = create_model(\n",
        "    X_train.shape[1],\n",
        "    best_params['hidden_units'],\n",
        "    best_params['learning_rate'],\n",
        "    best_params['dropout_rate']\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=best_params['epochs'],\n",
        "                    batch_size=best_params['batch_size'],\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stop])\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, digits=4)\n",
        "\n",
        "print(\"\\nFinal Test Accuracy:\", acc)\n",
        "print(\"Final F1 Score:\", f1)\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2Kf1BbImYce",
        "outputId": "9983765a-f5e6-4d49-e8f4-d88ae2b67197"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generation 1\n",
            "\n",
            "Generation 2\n",
            "\n",
            "Generation 3\n",
            "\n",
            "Generation 4\n",
            "\n",
            "Generation 5\n",
            "\n",
            "Best Parameters from GA: {'hidden_units': 256, 'learning_rate': 0.001, 'batch_size': 64, 'epochs': 40, 'dropout_rate': 0.4}\n",
            "Epoch 1/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6993 - loss: 0.5949 - val_accuracy: 0.7125 - val_loss: 0.5612\n",
            "Epoch 2/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7296 - loss: 0.5349 - val_accuracy: 0.7216 - val_loss: 0.5486\n",
            "Epoch 3/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7320 - loss: 0.5274 - val_accuracy: 0.7234 - val_loss: 0.5418\n",
            "Epoch 4/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7396 - loss: 0.5275 - val_accuracy: 0.7308 - val_loss: 0.5346\n",
            "Epoch 5/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7634 - loss: 0.5029 - val_accuracy: 0.7326 - val_loss: 0.5288\n",
            "Epoch 6/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7400 - loss: 0.5358 - val_accuracy: 0.7198 - val_loss: 0.5471\n",
            "Epoch 7/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7601 - loss: 0.4928 - val_accuracy: 0.7363 - val_loss: 0.5281\n",
            "Epoch 8/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7651 - loss: 0.4980 - val_accuracy: 0.7418 - val_loss: 0.5241\n",
            "Epoch 9/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7467 - loss: 0.5171 - val_accuracy: 0.7454 - val_loss: 0.5263\n",
            "Epoch 10/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7588 - loss: 0.5022 - val_accuracy: 0.7253 - val_loss: 0.5221\n",
            "Epoch 11/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7529 - loss: 0.5007 - val_accuracy: 0.7399 - val_loss: 0.5150\n",
            "Epoch 12/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7695 - loss: 0.4929 - val_accuracy: 0.7491 - val_loss: 0.5150\n",
            "Epoch 13/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7626 - loss: 0.4952 - val_accuracy: 0.7381 - val_loss: 0.5194\n",
            "Epoch 14/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7651 - loss: 0.4877 - val_accuracy: 0.7564 - val_loss: 0.5095\n",
            "Epoch 15/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7543 - loss: 0.5020 - val_accuracy: 0.7564 - val_loss: 0.5057\n",
            "Epoch 16/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7422 - loss: 0.5012 - val_accuracy: 0.7582 - val_loss: 0.5083\n",
            "Epoch 17/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7535 - loss: 0.4940 - val_accuracy: 0.7527 - val_loss: 0.5073\n",
            "Epoch 18/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7674 - loss: 0.4990 - val_accuracy: 0.7473 - val_loss: 0.5072\n",
            "Epoch 19/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7544 - loss: 0.4941 - val_accuracy: 0.7582 - val_loss: 0.5002\n",
            "Epoch 20/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7635 - loss: 0.4919 - val_accuracy: 0.7454 - val_loss: 0.5122\n",
            "Epoch 21/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7620 - loss: 0.4939 - val_accuracy: 0.7564 - val_loss: 0.4993\n",
            "Epoch 22/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7640 - loss: 0.4759 - val_accuracy: 0.7601 - val_loss: 0.4950\n",
            "Epoch 23/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7597 - loss: 0.4823 - val_accuracy: 0.7491 - val_loss: 0.4991\n",
            "Epoch 24/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7678 - loss: 0.4800 - val_accuracy: 0.7509 - val_loss: 0.4997\n",
            "Epoch 25/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7927 - loss: 0.4675 - val_accuracy: 0.7656 - val_loss: 0.4943\n",
            "Epoch 26/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7788 - loss: 0.4657 - val_accuracy: 0.7564 - val_loss: 0.4944\n",
            "Epoch 27/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7857 - loss: 0.4680 - val_accuracy: 0.7729 - val_loss: 0.4916\n",
            "Epoch 28/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7794 - loss: 0.4733 - val_accuracy: 0.7656 - val_loss: 0.4866\n",
            "Epoch 29/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7747 - loss: 0.4722 - val_accuracy: 0.7582 - val_loss: 0.4971\n",
            "Epoch 30/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7777 - loss: 0.4643 - val_accuracy: 0.7637 - val_loss: 0.4873\n",
            "Epoch 31/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7862 - loss: 0.4607 - val_accuracy: 0.7582 - val_loss: 0.4878\n",
            "Epoch 32/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7893 - loss: 0.4515 - val_accuracy: 0.7692 - val_loss: 0.4794\n",
            "Epoch 33/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7822 - loss: 0.4589 - val_accuracy: 0.7656 - val_loss: 0.4846\n",
            "Epoch 34/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7809 - loss: 0.4540 - val_accuracy: 0.7656 - val_loss: 0.4752\n",
            "Epoch 35/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7940 - loss: 0.4452 - val_accuracy: 0.7619 - val_loss: 0.4771\n",
            "Epoch 36/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7794 - loss: 0.4577 - val_accuracy: 0.7711 - val_loss: 0.4783\n",
            "Epoch 37/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7734 - loss: 0.4777 - val_accuracy: 0.7747 - val_loss: 0.4842\n",
            "Epoch 38/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7664 - loss: 0.4639 - val_accuracy: 0.7619 - val_loss: 0.4792\n",
            "Epoch 39/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7938 - loss: 0.4485 - val_accuracy: 0.7784 - val_loss: 0.4739\n",
            "Epoch 40/40\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7953 - loss: 0.4406 - val_accuracy: 0.7509 - val_loss: 0.4839\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\n",
            "Final Test Accuracy: 0.7756598240469208\n",
            "Final F1 Score: 0.7823613086770982\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       False     0.7864    0.7515    0.7685       338\n",
            "        True     0.7660    0.7994    0.7824       344\n",
            "\n",
            "    accuracy                         0.7757       682\n",
            "   macro avg     0.7762    0.7754    0.7754       682\n",
            "weighted avg     0.7761    0.7757    0.7755       682\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from imblearn.combine import SMOTETomek\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('pc1.csv')\n",
        "data.dropna(inplace=True)\n",
        "X = data.drop(['defects'], axis=1).values\n",
        "y = data['defects'].values\n",
        "\n",
        "# Resample using SMOTE-Tomek\n",
        "smt = SMOTETomek(random_state=42)\n",
        "X_resampled, y_resampled = smt.fit_resample(X, y)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_resampled = scaler.fit_transform(X_resampled)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# GA + ANN functions\n",
        "def create_model(input_dim, hidden_units, learning_rate, dropout_rate):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(hidden_units, activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(hidden_units // 2, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def GA_tune(X, y, population_size=6, generations=5):\n",
        "    input_dim = X.shape[1]\n",
        "\n",
        "    # Extended search space\n",
        "    hidden_units_list = [32, 64, 128, 256]\n",
        "    learning_rates = [0.0005, 0.001, 0.0025, 0.005, 0.01]\n",
        "    batch_sizes = [16, 32, 64, 128]\n",
        "    epochs_list = [20, 30, 40]\n",
        "    dropout_rates = [0.1, 0.2, 0.3, 0.4]\n",
        "\n",
        "    def random_individual():\n",
        "        return {\n",
        "            'hidden_units': random.choice(hidden_units_list),\n",
        "            'learning_rate': random.choice(learning_rates),\n",
        "            'batch_size': random.choice(batch_sizes),\n",
        "            'epochs': random.choice(epochs_list),\n",
        "            'dropout_rate': random.choice(dropout_rates)\n",
        "        }\n",
        "\n",
        "    def fitness(indiv):\n",
        "        model = create_model(input_dim, indiv['hidden_units'], indiv['learning_rate'], indiv['dropout_rate'])\n",
        "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        model.fit(X_train, y_train,\n",
        "                  epochs=indiv['epochs'],\n",
        "                  batch_size=indiv['batch_size'],\n",
        "                  verbose=0,\n",
        "                  validation_split=0.2,\n",
        "                  callbacks=[early_stop])\n",
        "        _, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "        return acc\n",
        "\n",
        "    # Initial population\n",
        "    population = [random_individual() for _ in range(population_size)]\n",
        "\n",
        "    for gen in range(generations):\n",
        "        print(f\"\\nGeneration {gen+1}\")\n",
        "        fitness_scores = [fitness(indiv) for indiv in population]\n",
        "        sorted_pop = [x for _, x in sorted(zip(fitness_scores, population), key=lambda pair: pair[0], reverse=True)]\n",
        "\n",
        "        new_population = sorted_pop[:2]  # Elitism\n",
        "        while len(new_population) < population_size:\n",
        "            p1, p2 = random.sample(sorted_pop[:4], 2)\n",
        "            child = {\n",
        "                'hidden_units': random.choice([p1['hidden_units'], p2['hidden_units']]),\n",
        "                'learning_rate': random.choice([p1['learning_rate'], p2['learning_rate']]),\n",
        "                'batch_size': random.choice([p1['batch_size'], p2['batch_size']]),\n",
        "                'epochs': random.choice([p1['epochs'], p2['epochs']]),\n",
        "                'dropout_rate': random.choice([p1['dropout_rate'], p2['dropout_rate']])\n",
        "            }\n",
        "            # Mutation\n",
        "            if random.random() < 0.2:\n",
        "                child['hidden_units'] = random.choice(hidden_units_list)\n",
        "            new_population.append(child)\n",
        "\n",
        "        population = new_population\n",
        "\n",
        "    best_individual = population[0]\n",
        "    print(\"\\nBest Parameters from GA:\", best_individual)\n",
        "    return best_individual\n",
        "\n",
        "# Run GA to find best parameters\n",
        "best_params = GA_tune(X_resampled, y_resampled)\n",
        "\n",
        "# Train final model using best params\n",
        "model = create_model(\n",
        "    X_train.shape[1],\n",
        "    best_params['hidden_units'],\n",
        "    best_params['learning_rate'],\n",
        "    best_params['dropout_rate']\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=best_params['epochs'],\n",
        "                    batch_size=best_params['batch_size'],\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stop])\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, digits=4)\n",
        "\n",
        "print(\"\\nFinal Test Accuracy:\", acc)\n",
        "print(\"Final F1 Score:\", f1)\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af8XyeMsp2XA",
        "outputId": "85f6e1cb-d760-43cb-e897-c81679aae2ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generation 1\n",
            "\n",
            "Generation 2\n",
            "\n",
            "Generation 3\n",
            "\n",
            "Generation 4\n",
            "\n",
            "Generation 5\n",
            "\n",
            "Best Parameters from GA: {'hidden_units': 256, 'learning_rate': 0.0025, 'batch_size': 64, 'epochs': 20, 'dropout_rate': 0.1}\n",
            "Epoch 1/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.6534 - loss: 0.5951 - val_accuracy: 0.8026 - val_loss: 0.4581\n",
            "Epoch 2/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7956 - loss: 0.4216 - val_accuracy: 0.8355 - val_loss: 0.3928\n",
            "Epoch 3/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8297 - loss: 0.3766 - val_accuracy: 0.8947 - val_loss: 0.3099\n",
            "Epoch 4/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8751 - loss: 0.3166 - val_accuracy: 0.8717 - val_loss: 0.3196\n",
            "Epoch 5/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8638 - loss: 0.3158 - val_accuracy: 0.9013 - val_loss: 0.2820\n",
            "Epoch 6/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8958 - loss: 0.2986 - val_accuracy: 0.8882 - val_loss: 0.2930\n",
            "Epoch 7/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8643 - loss: 0.2903 - val_accuracy: 0.8849 - val_loss: 0.2755\n",
            "Epoch 8/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8789 - loss: 0.2848 - val_accuracy: 0.9013 - val_loss: 0.2606\n",
            "Epoch 9/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9073 - loss: 0.2428 - val_accuracy: 0.8980 - val_loss: 0.2634\n",
            "Epoch 10/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8923 - loss: 0.2604 - val_accuracy: 0.9046 - val_loss: 0.2522\n",
            "Epoch 11/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9008 - loss: 0.2426 - val_accuracy: 0.9112 - val_loss: 0.2416\n",
            "Epoch 12/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8992 - loss: 0.2605 - val_accuracy: 0.9112 - val_loss: 0.2259\n",
            "Epoch 13/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9066 - loss: 0.2336 - val_accuracy: 0.9211 - val_loss: 0.2260\n",
            "Epoch 14/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9168 - loss: 0.2229 - val_accuracy: 0.9243 - val_loss: 0.2253\n",
            "Epoch 15/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9162 - loss: 0.2179 - val_accuracy: 0.8980 - val_loss: 0.2276\n",
            "Epoch 16/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9119 - loss: 0.2107 - val_accuracy: 0.9474 - val_loss: 0.2222\n",
            "Epoch 17/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9201 - loss: 0.2214 - val_accuracy: 0.9243 - val_loss: 0.2056\n",
            "Epoch 18/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9164 - loss: 0.2327 - val_accuracy: 0.9112 - val_loss: 0.1953\n",
            "Epoch 19/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9143 - loss: 0.2135 - val_accuracy: 0.9342 - val_loss: 0.1812\n",
            "Epoch 20/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9298 - loss: 0.1896 - val_accuracy: 0.9276 - val_loss: 0.1796\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\n",
            "Final Test Accuracy: 0.9078947368421053\n",
            "Final F1 Score: 0.913151364764268\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       False     0.9817    0.8342    0.9020       193\n",
            "        True     0.8519    0.9840    0.9132       187\n",
            "\n",
            "    accuracy                         0.9079       380\n",
            "   macro avg     0.9168    0.9091    0.9076       380\n",
            "weighted avg     0.9178    0.9079    0.9075       380\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OmsOyzoztSIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}